---
title: "Session notes day 3"
author: "Tim Riffe"
date: '2022-07-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tuesday's worked example

I'll start by condensing the progress we made as of yesterday, so that we have everything in one place.

Load some packages we'll probably need:
```{r}
library(tidyverse)
library(haven)
library(janitor)
```

Read in the data. How big is it? What column-naming conventions does it follow?
```{r}
tu <- read_sav("Data/caseid_aggr.sav")
dim(tu)

head(tu[, 1:20])
```

Siphon off the activity codes and labels, as we'll want these to make sense of the data later.
```{r}
activities <- 
  tu %>% 
  # pull() extracts a column to a vector
  pull(time_300_max) %>% 
  
  # unique() asks what are the actual values present
  # in the vector
  unique() %>% 
  
  # extract the label attribute. Now we have a vector
  # of codes (numeric) with activity labels, seems like
  # it could be useful
  attr("labels")

activities
```

We should down-sample this in order to develop the pipeline. Then pivot to longer

```{r}
60 * 24
colnames(tu) %>% rev()
set.seed(1)
tu2 <- 
  tu %>% 
  # I made this even smaller
  sample_n(600) %>% 
  # Now pivot longer and just keep the columns we need to move on
  pivot_longer(ends_with("max") & starts_with("time"),
               # contains("max"),
               #time_1_max:(ncol(.)-1),
               #time_1_max:time_1440_max,
               names_to = "time",
               values_to = "activity") %>% 
  select(WT06, time, activity) %>% 
  # pick out the integer in the middle of the names
  mutate(time = parse_number(time)) %>% 
  # pick out every 10th minute (optional, it's just a choice)
  filter(time %% 10 == 1) %>% 
  # declare independent groups of time and activity
  group_by(time, activity) %>% 
  # aggregate weights within these groups
  summarize(n = sum(WT06)) %>% 
  # now group by time in order to calculate 
  # frequencies at each time point
  group_by(time) %>% 
  # calculate frequencies
  mutate(freq = n / sum(n)) %>% 
  ungroup() %>% 
  mutate(activity = factor(activity,
                      levels = activities,
                      labels = names(activities)))
tu2
```

## now plot it!

```{r}
library(colorspace)
tu2 %>% 
  ggplot(mapping = aes(x = time, y = freq, fill = activity)) +
  geom_area() +
  # scale_fill_brewer(type = "qual")
  scale_fill_discrete_qualitative("Set3")
```

How we calculate relative frequencies, aka proportions or fractions. This is of course premised on positive values. This is what we're doing with weights, where each activity has a weight, and we repeat this step within each time point.
```{r}
set.seed(1)
a <- runif(10)
a / sum(a)
```

### explicit factor reordering

Here's a trick to do that; I we use `fct_relevel()` from the `forcats` package (which actually already was loaded via `tidyverse`). Just be sure to list all the labels in the order you want them stacked.

```{r}
library(forcats)
activities
tu2 %>% 
  mutate(activity = fct_relevel(activity, c("Eat", "Childcare activities", "Care adult", "Personal care", "TV","Leisure","Housework", "Travel","Work", "Education","Other"))) %>% 
  ggplot(mapping = aes(x = time, y = freq, fill = activity)) +
  geom_area() +
  # scale_fill_brewer(type = "qual")
  scale_fill_discrete_qualitative("Set3")
```




### using `set.seed()`

A note on random number generation. As we were producing the above pipeline, we were all ending up with different numbers of rows in the final dataset. This was because `sample_n()` takes a random sample of rows, and we were all getting different random numbers! The `set.seed()` function can be used to anchor the random number series at a certain point, which can ensure we all get the same random numbers.
```{r}
set.seed(2022)
sample(letters, 10, replace = FALSE)
# different
sample(letters, 10, replace = FALSE)
set.seed(2022)
# same again
sample(letters, 10, replace = FALSE)
```
Using `set.seed()`, we were therefore able to ensure identical results. In the end, you would remove the random row selection anyway, for an unfiltered result, but this is a useful trick in some other instances, especially simulation results.

### modulo
A note on using modulo operator to select rows. It can be handy from time to time.
```{r}
A <- tibble(a = letters[1:10], b = 0:9)
A$b %% 3 == 0
A %>% 
  filter(b %% 3 == 0)

age <- 0:100
age - age %% 5

tibble(deaths = rpois(101, lambda = 50),
       age = 0:100) %>% 
  mutate(age5 = age - age %% 5) %>% 
  group_by(age5) %>% 
  summarize(deaths = sum(deaths))
```

## Functions

The main pieces of a function, when we write our own.
1. it's name, which is just a subjective choice, but it's a good idea to make function names that are somehow easy to predict what the function is going to do. Or that are somehow intuitive.
2. assign `function(){}` to it.
3. comma-separate your arguments in `()`
4. choose argument names that make sense to you and others.
5. the arguments are free variables that you can use and work with inside the function body.
6. everything inside of `{}` is the function body.
7. what happens inside `{}` is free for you to choose. It could be ugly. It could be parsimonious. it doesn't matter, as long as it produces the result you need. In a reliable way.
8. `return()` specifies what the function will spit out.

```{r}
hello <- function(your_name){
  
  my_message <- paste0("Hello ", your_name,". Have a nice day!")
  return(my_message)
}

hello("Harry Potter")
paste0("a", "b", "c")
```

```{r}
a_numerical_example <- function(x, y, z){
  x * y + z ^ 2
}
a_numerical_example(45,2,.9)


```

## The lifetable identities

```{r}

omega <- 110
x     <- 0:omega
a     <- 0.00022
b     <- 0.07
mx    <- a * exp(x * b)
ax    <- rep(.5, length(mx))
```
$m(x)$ we invent to have something to work with. It stands for mortality rates ina ge intervals.

$a(x)$ is an instrumental variable. It is interpreted as the average time spent in an age interval by those that don't make it to the end. We use it to convert $m(x)$ into $q(x)$.

$q(x)$ is the probability of dying in an age interval, give that we survive to the start of the age interval.

$$ q(x) = \frac{m(x)}{1 + (1 - a(x)) \cdot m(x)}$$
```{r}
mx_to_qx <- function(mx, ax){
  qx <- mx / (1 + (1 - ax) * mx)
  return(qx)
}
# demonstrate usage:
qx <- mx_to_qx(mx, ax)
```

The probabilities are a bit easier to work with to derive the remaining columns of the lifetable. We can convert them to survival probabilities $p(x)$ by just taking the complement of $q(x)$

$$  p(x) = 1 - q(x) $$
```{r}
qx_to_px <- function(qx){
  px <- 1 - qx
  return(px)
}
px <- qx_to_px(qx)
# mx_to_qx(mx, ax) %>% 
#   qx_to_px()

```


Survivorship, i.e. the probability of surviving from birth until a given age, $\ell(x)$

$$ \ell(x)  =\prod_{i=0}^{x-1} p(i)$$
There a function called `cumprod()` that you might want to check out

```{r}
px_to_lx <- function(px, radix = 1){
  n <- length(px)
  lx <- c(1,cumprod(px))
  
  # this step is bookkeeping, making sure the output
  # vector is of the same length.
  lx <- lx[1:n]
  return(radix * lx)
}
lx <- px_to_lx(px)
```



Lifetable deaths, $d(x)$, are:

$$ d(x) = \ell(x) \cdot q(x)$$

Two versions: first, which starts from $q(x)$, then it uses the little functions we already made to jump all the way to $d(x)$. The next one assumes we already have the pieces we need ($q(x)$ and $d(x)$).
```{r}
qx_to_dx <- function(qx, radix = 1){
  px <- qx_to_px(qx)
  lx <- px_to_lx(px = px, radix = radix)
  dx <- qx * lx
  return(dx)
}

qxlx_to_dx <- function(qx, lx){
  dx <- qx * lx
  return(dx)
}

```

$L(x)$ is lifetable **exposure**, i.e. the total years lived in an age interval by those that start in the interval.
$$ L(x) = \ell(x) -(1 - a(x)) \cdot d(x)$$

```{r}
lxdx_to_Lx <- function(lx, dx, ax){
  Lx <- lx - (1 - ax) * dx
  return(Lx)
}
lx <- px_to_lx(px)
dx <- qx_to_dx(qx)
Lx <- lxdx_to_Lx(lx = lx, ax = ax, dx = dx)
```

You can get back the rates we started with like so: (no need to implement this, but you can check if you want)
$$ m(x) = d(x) \cdot L(x)$$
$T(x)$ are all the years left to be lived by those alive at a given age. The remaining years of life of the whole lifetable population at a particular age.

$$T(x) = \sum_{i=x}^\omega L(i)$$

```{r}
Lx %>% rev() %>% cumsum() %>% rev()
Lx_to_Tx <- function(Lx){
  Tx <- Lx %>% 
    rev() %>% 
    cumsum() %>% 
    rev()
  return(Tx)
}
```

Life expectancy at each age is $e(x)$, is calculated as:

$$e(x) = \frac{T(x)}{\ell(x)}$$
```{r}
Txlx_to_ex <- function(Tx, lx){
  ex <- Tx / lx
  return(ex)
}
Tx <- Lx_to_Tx(Lx)
Txlx_to_ex(Tx, lx)
```


Let's piece these little steps into a full-service lifetable function.

We have written direct implementations of the standard step-by-step lifetable calculation steps. We talked about what the different transformations mean, but really this is all about rote implementation of prescribed formulas. And we should get a sense of how straightforward it might be to create a function to implement someone's formula in a paper. Indeed, that might be a path to understand the method. An alternative, or complement to paper and pencil. Plus if you do it, then you can visualize as you go and get a sense of things.

So, or strategy was NOT to look at the entirety of the lifetable functions at once, but rather to do it one piece at a time, and to understand things one step at a time. We then use the functions together in our tidyverse framework to show how code modularity can make big tasks into small ones.

```{r}
lifetable <- tibble(age = x,
                    mx = mx,
                    ax = ax) %>% 
  mutate(qx = mx_to_qx(mx = mx, ax = ax),
         px = qx_to_px(qx = qx),
         lx = px_to_lx(px = px, radix = 100000),
         dx = qxlx_to_dx(qx = qx, lx = lx),
         Lx = lxdx_to_Lx(lx = lx, dx = dx, ax = ax),
         Tx = Lx_to_Tx(Lx),
         ex = Txlx_to_ex(Tx = Tx, lx = lx))
lifetable %>% 
  ggplot(aes(x = age, 
             y = ex)) +
  geom_line() 
```













